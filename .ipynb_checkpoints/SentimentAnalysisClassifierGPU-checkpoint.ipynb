{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ttb-folio/Pitchfork-Reviews/blob/main/SentimentAnalysisClassifierGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LENDdMahAd2",
    "outputId": "609674ca-de73-47c7-bfc1-d7273ae07bad"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !git clone https://github.com/ttb-folio/Pitchfork-Reviews.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gwWXhhf-gWI_"
   },
   "outputs": [],
   "source": [
    "\n",
    "content_df = pd.read_csv('Data/content.csv', nrows = 10)\n",
    "review_df = pd.read_csv('Data/reviews.csv', nrows = 10)\n",
    "reviews = content_df.content.fillna(' ').values\n",
    "labels = (review_df.score <= 6).astype(int).values\n",
    "\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235,
     "referenced_widgets": [
      "b9092cb6bfed44f181e22c3e33be6bd8",
      "62786836aa534c28b0bcace022361552",
      "00ea7c3c54c0443494c22cb137fd0a73",
      "23e92002b12e42be9b26530b4f740b8e",
      "ef4e0ee90da64dc6966aa60f0f27d86c",
      "f2fc6d53352f4c6fba1b78af87431534",
      "5205c88e70c845529bbd6bf58b4ff2a9",
      "49a2a7f1b3bb49f381a67e958818d08e",
      "d46122f07afa4c09a42222dbb592b0ad",
      "a23de6e2cc884ece9a389767153e3f19",
      "36423e60e056403ebce7d7851b838c8d",
      "ccaea8639e164503b45bea9deb5d90cc",
      "48055a56c33c4d8e835af2760e450fd2",
      "d71dce6973034b04890c189c393ec01e",
      "beffe4a4ee5e4b60977f47ec631beb7d",
      "a37bce5d99514843b0db3403983aad46",
      "7f2a90ee108b48258ef407c6be22302e",
      "5c9316e621324ebb84a794a63737fff7",
      "9ec8fb94bd8b4325b491cb29f8366fb0",
      "9dbb489d6bc543fa860295d4f1772548",
      "3459b8491b514d1abb6c48959e00596a",
      "49747ab5d4a144dabd9e610bfa04d37e",
      "1010f13992f342c2a471e6cf919134cf",
      "08796c2378c34273909c687e15f23736",
      "51b9544a54ce4d2eadb0df71c7b8586f",
      "7dff024bc83b45a4b8305b851dd0ad32",
      "c01e5c2aa98f44b39f7ca054f2507ef1",
      "9d389861befe4ca9b019cec4fc66491d",
      "a2d433fd4fa74582927bf55bdbf31a00",
      "359e9ce85cda46e88cb0963cca604f77",
      "da0414b04a344364afb4cdbf471fdc75",
      "30dc920a9df44393b5adc06e516f2286"
     ]
    },
    "id": "_A8p-u1WRROT",
    "outputId": "50ffbf65-d927-4760-d4d1-cec079214932"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize(x_text):\n",
    "    text = list(x_text)\n",
    "    tokenizedText = tokenizer(text, padding = True, truncation = True)\n",
    "    return tokenizedText\n",
    "\n",
    "x_train = tokenize(reviews_train)\n",
    "x_test = tokenize(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = tokenizer(list(reviews_train), padding = True, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2 == x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1999, 2047, 2259, 1521, 1055, 6388, 3496, 1010, 1996, 3739, 1997, 8032, 18904, 8322, 4845, 3286, 5638, 2006, 1037, 4164, 3021, 2038, 2042, 1037, 8335, 17245, 1997, 3737, 1012, 1999, 2286, 1010, 2016, 7622, 1999, 1037, 2444, 3311, 2007, 1996, 8987, 4543, 1998, 17727, 12298, 17288, 14163, 8865, 2957, 23063, 1012, 2012, 1996, 2297, 9809, 20313, 1010, 2016, 2001, 2112, 1997, 1037, 22446, 7241, 2008, 2435, 1996, 6765, 1997, 2028, 1997, 1996, 2345, 14281, 2011, 1996, 2397, 28036, 2728, 9321, 1012, 2127, 5716, 1996, 2177, 6422, 3924, 1010, 2295, 1010, 4845, 3286, 5638, 2018, 2664, 2000, 3443, 1037, 4316, 2005, 2014, 2219, 9265, 1012, 1996, 8530, 2008, 3248, 2006, 2014, 2316, 19738, 4667, 2834, 2950, 2070, 5220, 11725, 1012, 7101, 4098, 14855, 16020, 1521, 1055, 14855, 23200, 2373, 2038, 2042, 2657, 1999, 1996, 2152, 1011, 11619, 3769, 1997, 1996, 2316, 5841, 1012, 10430, 19977, 4717, 5912, 1521, 1055, 2147, 2038, 4928, 4187, 2000, 5633, 2011, 8694, 1011, 4210, 1998, 2645, 16588, 1012, 2021, 4845, 3286, 5638, 2003, 1996, 3154, 4439, 4005, 1997, 1996, 2177, 1012, 2096, 2014, 3262, 2773, 3238, 5554, 3033, 2006, 4151, 2671, 2024, 5105, 2011, 1996, 2148, 2796, 13347, 2389, 4823, 2967, 2016, 4194, 1999, 2004, 1037, 2775, 1010, 2027, 2036, 2655, 2000, 2568, 2014, 2627, 2147, 2007, 4543, 14861, 3557, 1012, 4845, 3286, 5638, 1521, 1055, 17424, 25546, 5007, 2652, 11138, 2014, 7552, 2817, 1997, 2634, 1521, 1055, 2482, 19833, 2594, 4556, 4535, 1010, 2004, 2092, 2004, 2014, 12284, 1997, 2715, 18465, 2189, 1012, 1998, 1996, 3110, 1997, 2489, 4166, 1517, 1999, 3327, 1010, 1996, 2152, 1011, 8015, 8479, 1997, 2397, 9110, 18053, 1517, 2003, 2411, 2556, 2182, 1012, 2008, 1521, 1055, 1037, 2843, 1997, 3430, 2005, 2151, 4543, 2000, 2832, 6179, 2135, 1010, 2021, 4845, 3286, 5638, 1998, 6422, 3924, 10782, 3209, 2011, 11865, 7741, 2122, 8092, 2007, 1037, 2391, 1997, 3193, 2035, 2037, 2219, 1012, 2007, 2169, 12401, 2650, 4159, 2044, 1037, 9805, 3654, 1006, 2030, 1523, 1041, 2239, 1524, 1007, 1999, 7560, 18919, 1010, 4151, 2671, 4415, 2038, 3278, 23539, 19509, 1012, 2664, 4845, 3286, 5638, 1521, 1055, 4185, 1011, 3371, 7621, 3397, 17133, 1010, 3371, 1011, 2000, 1011, 3371, 3528, 1010, 1999, 2804, 2000, 1037, 2882, 3452, 2640, 1012, 1996, 2516, 1997, 1996, 3098, 2929, 1010, 1523, 2938, 17915, 9805, 3654, 1010, 1524, 7604, 2019, 3418, 1041, 2239, 1999, 7560, 11327, 1517, 2028, 1999, 2029, 6259, 16724, 2001, 6923, 1012, 1996, 2650, 7480, 2007, 1996, 3003, 1521, 1055, 2376, 1998, 25546, 5007, 2652, 1010, 1998, 1996, 14902, 2003, 19960, 29293, 1012, 4845, 3286, 5638, 1521, 1055, 4823, 2038, 1996, 2514, 1997, 25951, 1010, 2007, 3056, 2967, 1997, 20732, 4909, 7422, 23318, 1012, 2021, 2016, 2036, 9909, 2000, 1996, 3210, 2004, 2016, 3632, 2247, 1010, 4526, 1037, 6575, 3737, 1012, 2043, 14855, 16020, 8039, 1010, 2002, 3248, 1999, 2489, 1011, 2051, 1010, 2411, 2478, 22330, 11201, 4877, 2000, 9669, 20598, 1996, 5554, 3210, 1012, 2044, 5912, 1521, 1055, 10430, 19656, 9794, 1010, 3053, 2093, 2781, 1999, 1010, 2002, 1998, 4845, 3286, 5638, 4088, 23781, 1517, 2200, 5238, 1517, 2007, 5866, 28594, 1012, 2044, 102], [101, 2009, 1521, 1055, 2025, 3733, 2005, 7101, 2015, 2000, 2131, 1996, 17763, 1010, 2926, 2065, 2027, 1521, 2128, 1996, 2512, 1011, 4823, 2785, 1517, 2053, 3043, 2129, 4013, 4305, 11411, 27191, 10904, 2027, 2089, 2022, 1012, 2005, 2296, 6289, 14503, 1523, 8795, 14301, 2063, 1524, 3779, 1006, 1996, 6147, 1007, 2030, 11317, 18178, 1006, 2123, 9298, 24164, 3217, 1007, 1010, 2045, 2024, 5606, 1997, 10812, 2273, 1998, 2308, 2040, 9044, 1996, 2067, 3786, 2096, 2037, 2316, 15416, 2202, 2415, 2754, 1012, 2044, 2086, 1997, 2652, 2067, 1011, 2039, 2000, 2500, 1010, 2164, 2116, 1999, 1996, 2975, 1998, 4167, 7959, 14728, 2099, 3830, 26041, 2229, 1006, 1998, 1010, 16484, 1010, 10860, 2773, 20110, 3910, 5487, 14163, 12179, 8545, 2290, 1999, 2010, 3026, 25521, 2004, 1037, 4166, 2350, 1007, 1010, 7101, 9389, 7442, 2520, 1521, 21009, 2010, 2376, 2006, 3948, 2834, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 1010, 1037, 10889, 17087, 2501, 3605, 2471, 4498, 1997, 2010, 2219, 22980, 1012, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 3475, 1521, 1056, 1996, 2034, 2051, 3766, 2038, 2404, 2370, 2041, 2392, 1012, 3130, 2002, 2599, 1996, 4166, 1013, 5099, 18471, 1013, 2600, 2316, 9413, 9581, 3501, 1010, 2040, 2207, 2028, 2062, 2030, 2625, 17092, 2501, 1999, 2262, 1025, 2295, 2002, 2001, 1037, 4054, 6009, 1010, 2002, 2001, 2036, 2074, 1037, 2266, 1998, 2025, 1996, 2177, 1521, 1055, 3220, 1012, 2021, 2006, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 1010, 3766, 12970, 2185, 1996, 5693, 1998, 16106, 1010, 2975, 2369, 2069, 2010, 6333, 1010, 2119, 2444, 1998, 7077, 2098, 1010, 2035, 1996, 2096, 20540, 1037, 3622, 3160, 2055, 2054, 3599, 2465, 14144, 2004, 1037, 1523, 2299, 1012, 1524, 3766, 4748, 9691, 2015, 1037, 9210, 1997, 2010, 3162, 2007, 17187, 16222, 5833, 28578, 11187, 2021, 2702, 1997, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 1521, 1055, 5417, 7659, 2024, 2074, 1999, 1011, 2115, 1011, 2227, 6943, 4978, 1012, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 4269, 2007, 1037, 4629, 1010, 2048, 1011, 4375, 1055, 26148, 10245, 2006, 1523, 2040, 2097, 3233, 1029, 1524, 2008, 24814, 1996, 17289, 8795, 14301, 2063, 1998, 2010, 3098, 2000, 1996, 7117, 1521, 1055, 1523, 1996, 6534, 1006, 1016, 1012, 1014, 1007, 1012, 1524, 1996, 14402, 2003, 8478, 1010, 2021, 3766, 1521, 2544, 2003, 1037, 2962, 2094, 1010, 5292, 28431, 2680, 7101, 1011, 2006, 1011, 2010, 1011, 2219, 3218, 5219, 2008, 3957, 2126, 2000, 2330, 2686, 2612, 1997, 1037, 4389, 6348, 1998, 5132, 1012, 2009, 1521, 1055, 9487, 2129, 2107, 1037, 2460, 1055, 3490, 29519, 1997, 2614, 2064, 2681, 2107, 2019, 15738, 1010, 2021, 2009, 4520, 1996, 2754, 2005, 1013, 1013, 1013, 1013, 1013, 3466, 8787, 1999, 2019, 5976, 1998, 4621, 2126, 1012, 2054, 2087, 27343, 3766, 1521, 3162, 2003, 1037, 13939, 28322, 1998, 19118, 13181, 20200, 3737, 1010, 2081, 2062, 8478, 2011, 1996, 18001, 3405, 3737, 1012, 1996, 8331, 9265, 2823, 23408, 11045, 4438, 3948, 4616, 2066, 4098, 20997, 1521, 1055, 1523, 1996, 6943, 2036, 17569, 2229, 1524, 2030, 4116, 3766, 1521, 1523, 9052, 1524, 7121, 2041, 1998, 10641, 1012, 1523, 7276, 7277, 1010, 1524, 2007, 2049, 20605, 4667, 1010, 102], [101, 3157, 4960, 10063, 3040, 23356, 7990, 2128, 2480, 12131, 2038, 2985, 5109, 6218, 2075, 2055, 1996, 2189, 2449, 1010, 5306, 2067, 2000, 2010, 10821, 2055, 2694, 2102, 1999, 2826, 1998, 2010, 4525, 1523, 3595, 3405, 6521, 1524, 1997, 1996, 3714, 4958, 1012, 2085, 1999, 2070, 3971, 1010, 2002, 2003, 1996, 2189, 2449, 1010, 1037, 2373, 2447, 3005, 13280, 5829, 1517, 4474, 7085, 1010, 6034, 20259, 1010, 5470, 15058, 13142, 1010, 2502, 5166, 3293, 6050, 5841, 1517, 2031, 2468, 3795, 1011, 3769, 1011, 2732, 1055, 1012, 1051, 1012, 1052, 1012, 2061, 2043, 2002, 7782, 2135, 13999, 2010, 4474, 2047, 4958, 2025, 1996, 5025, 2824, 2004, 1523, 2019, 4895, 19699, 9013, 18718, 1010, 7199, 17727, 8625, 6494, 3468, 2501, 2008, 2057, 2734, 2000, 2191, 1524, 2045, 2003, 2070, 3426, 2005, 2119, 20014, 27611, 1998, 7965, 27936, 1012, 2005, 11155, 8771, 1997, 2128, 2480, 12131, 1010, 1037, 2261, 16820, 6592, 3209, 1012, 2672, 2002, 1005, 1055, 5327, 2000, 13299, 12024, 2005, 1037, 7263, 1010, 2538, 1011, 3371, 4958, 2008, 3701, 4240, 2004, 1037, 10319, 6994, 2005, 1037, 19817, 21818, 1997, 16483, 17173, 2015, 1012, 2672, 2002, 6732, 2002, 1005, 1055, 2589, 2242, 9487, 1010, 2138, 2002, 2145, 5927, 2370, 2004, 2019, 7601, 7103, 4263, 1010, 2130, 2295, 2010, 6434, 2144, 29455, 9152, 2078, 1999, 2384, 2038, 2042, 2092, 1011, 14902, 2094, 2021, 2593, 18579, 5675, 2594, 1006, 2007, 4091, 1010, 13431, 6017, 1010, 1996, 7540, 1005, 1055, 2034, 2431, 1007, 2030, 22502, 12479, 1006, 11277, 1045, 1011, 4921, 1010, 1996, 2117, 2431, 1997, 1996, 7540, 1010, 3033, 1997, 2095, 5717, 1007, 1012, 23569, 27605, 12837, 1998, 3280, 11783, 2015, 2453, 4299, 2005, 1037, 2353, 5724, 1024, 2672, 2002, 1005, 1055, 11476, 2135, 2550, 3928, 1998, 4840, 2189, 2104, 1996, 3157, 4960, 10063, 9484, 1012, 2000, 2128, 2480, 12131, 1521, 1055, 4923, 1998, 20010, 20026, 4765, 1010, 2002, 1005, 1055, 3266, 2000, 3543, 2006, 2169, 11967, 1012, 2045, 2024, 2069, 1037, 9210, 1997, 4973, 1999, 2128, 2480, 12131, 1521, 1055, 2695, 1011, 27620, 2140, 9152, 2078, 6434, 2073, 1996, 2177, 2031, 7745, 2013, 2037, 22609, 1010, 24646, 10867, 1011, 6151, 1011, 2852, 5654, 3919, 2964, 1012, 2045, 1521, 1055, 1996, 3682, 1998, 29536, 16044, 2099, 1011, 5533, 12532, 8659, 8022, 2121, 1523, 2035, 1996, 2293, 1999, 1996, 2088, 1010, 1524, 16181, 2000, 1996, 4728, 1011, 11868, 3238, 2007, 4091, 1025, 1996, 24067, 2100, 1010, 2058, 10052, 1998, 2104, 1011, 17776, 6150, 2015, 1011, 2069, 9346, 1011, 20133, 11277, 1045, 1011, 4921, 1025, 1998, 2006, 2286, 1521, 1055, 13431, 6017, 1010, 1996, 8670, 4246, 2989, 1010, 11559, 1523, 2673, 1010, 1524, 1037, 4678, 2350, 1011, 3145, 8694, 1999, 1996, 2316, 1521, 1055, 12105, 1012, 1996, 2062, 5875, 1997, 2122, 1010, 1523, 2035, 1996, 2293, 1999, 1996, 2088, 1524, 1998, 1523, 2673, 1010, 1524, 2024, 1996, 4500, 1997, 1523, 4895, 19699, 9013, 18718, 1524, 2030, 1523, 17727, 8625, 6494, 3468, 1524, 1517, 2037, 4487, 10286, 6562, 8251, 2003, 2054, 3084, 2068, 13432, 1012, 3157, 4960, 10063, 2031, 2985, 3053, 4228, 2086, 6202, 2006, 1037, 8085, 2828, 1997, 11113, 8180, 3512, 1010, 3008, 1011, 16360, 23918, 3919, 102], [101, 2355, 2038, 2042, 1037, 16524, 1998, 18691, 2095, 2005, 2087, 1010, 2021, 2054, 1037, 1059, 11961, 2140, 11101, 2009, 2442, 2031, 2042, 2005, 11463, 3981, 4241, 3334, 2618, 1012, 1996, 2624, 3799, 5455, 2040, 10438, 2004, 6108, 2061, 2213, 1006, 1037, 21937, 3574, 1523, 3377, 4231, 1524, 1998, 2766, 2013, 1037, 3336, 2171, 13103, 1037, 2474, 24282, 11721, 14905, 5740, 1007, 7255, 2007, 10210, 5488, 1998, 2887, 6350, 1010, 2207, 1037, 1021, 1524, 2006, 6638, 13433, 4757, 2819, 1010, 2441, 2005, 2848, 24998, 1998, 2198, 1010, 2772, 2000, 26572, 6371, 8516, 1010, 1998, 2038, 2042, 2551, 2006, 1037, 2834, 6948, 1012, 2023, 19857, 12244, 1997, 4023, 2003, 4321, 1996, 2765, 1997, 1037, 10247, 2100, 3247, 2081, 2006, 15060, 2325, 1010, 2043, 4241, 3334, 2618, 27491, 3333, 1037, 3157, 1011, 2650, 3074, 1997, 1523, 14342, 1998, 2736, 2774, 1524, 3031, 2316, 26468, 2104, 1996, 2171, 24819, 1012, 1523, 2009, 2001, 3294, 4895, 24759, 20147, 2094, 1010, 1524, 4241, 3334, 2618, 2409, 8305, 1012, 1523, 1045, 2134, 1521, 1056, 2130, 2228, 2055, 1996, 2650, 10328, 2030, 1996, 2201, 8266, 2030, 1996, 2344, 1997, 2774, 1012, 1045, 2031, 1037, 10228, 1997, 2122, 2774, 1010, 1998, 2013, 2068, 1045, 3856, 3157, 1012, 1524, 2008, 2122, 3162, 1010, 2101, 2128, 21309, 2004, 2735, 2046, 1010, 2024, 10892, 18267, 2003, 6827, 2000, 4824, 2339, 1996, 3074, 2038, 2042, 2128, 1011, 2207, 3807, 1010, 2034, 2011, 2327, 11142, 1998, 2101, 2011, 26572, 6371, 8516, 1012, 2169, 2650, 2003, 1037, 6171, 12853, 5378, 2008, 11138, 4241, 3334, 2618, 1521, 1055, 2627, 3315, 3325, 1998, 19939, 2015, 2014, 10015, 2925, 1012, 2016, 3473, 2039, 2652, 1996, 9368, 1998, 3740, 2000, 5463, 1037, 11879, 2565, 2044, 2152, 2082, 2000, 7323, 2014, 2293, 1997, 4166, 1012, 2612, 1010, 2044, 9301, 2008, 2016, 2359, 2000, 3613, 14029, 1010, 2016, 8302, 2012, 2451, 2267, 2000, 2817, 2189, 2537, 1998, 3405, 1012, 2016, 2357, 2014, 5010, 2046, 1037, 2996, 1010, 2130, 9268, 1996, 2793, 2000, 16500, 1037, 6943, 2275, 1998, 2404, 14050, 2105, 2007, 1996, 3162, 2008, 2052, 2468, 2735, 2046, 1012, 2016, 3248, 2296, 6602, 2006, 1996, 2201, 1010, 1037, 8658, 2008, 2089, 10825, 2070, 1997, 4074, 1043, 1521, 1055, 2188, 5633, 1010, 1999, 2804, 2000, 6809, 1998, 11495, 1012, 2735, 2046, 14590, 2125, 2007, 1523, 18237, 2879, 1010, 1524, 1037, 10556, 23057, 12269, 26461, 3959, 1011, 3769, 2193, 3794, 16299, 2955, 1998, 21274, 22894, 1011, 22894, 2858, 3896, 1006, 23263, 1010, 6195, 4241, 3334, 2618, 1521, 1055, 2381, 1010, 2220, 22894, 15749, 2015, 2020, 2170, 1523, 12085, 16075, 1524, 1055, 2044, 1996, 4166, 28220, 1007, 1012, 8044, 2693, 1999, 2005, 1523, 5745, 1524, 1998, 1996, 2154, 16200, 10631, 3070, 16106, 2735, 24067, 2100, 2004, 4241, 3334, 2618, 17628, 2014, 10069, 1012, 1523, 2108, 6015, 2003, 1037, 4121, 4323, 1997, 1996, 2201, 1010, 1524, 4241, 3334, 2618, 2409, 2624, 3799, 1521, 1055, 1047, 4160, 2098, 1010, 1998, 2130, 2049, 2087, 27999, 1010, 2122, 2774, 5383, 2104, 10841, 14343, 7666, 1997, 10089, 1012, 2045, 2024, 5312, 2006, 1996, 2201, 2073, 2017, 2064, 2514, 2014, 20075, 6321, 6183, 2114, 2122, 28054, 1010, 2066, 2006, 1523, 102], [101, 2043, 3382, 1996, 10687, 2864, 1523, 4465, 9485, 1524, 2007, 9389, 11733, 5249, 1998, 19332, 16562, 2140, 2012, 1996, 2120, 4234, 3392, 7497, 5103, 1010, 2045, 2001, 2242, 19517, 2569, 1010, 1998, 6259, 1010, 2055, 2009, 1024, 1037, 4013, 4305, 11411, 2271, 1998, 15138, 2100, 1523, 18713, 10687, 1524, 4823, 2715, 26130, 1998, 27128, 2006, 1996, 2120, 2754, 2012, 1996, 2022, 15689, 2102, 1997, 1996, 2034, 2304, 2343, 1997, 1996, 2142, 2163, 1999, 2010, 2197, 2420, 1025, 1996, 19143, 1997, 1037, 2643, 3959, 1998, 1996, 2137, 3959, 1012, 2009, 2371, 2066, 2242, 2000, 2061, 4817, 1999, 1998, 7842, 14550, 1010, 2000, 2131, 2053, 9496, 4095, 3672, 2013, 1517, 2119, 2138, 1997, 2049, 2471, 18691, 2135, 2039, 26644, 2943, 1998, 2138, 2009, 2001, 2242, 2057, 2453, 2196, 2156, 2153, 1010, 2006, 2151, 4175, 1012, 3382, 2790, 2000, 26577, 1999, 1996, 2617, 1010, 2004, 2065, 14622, 1998, 2128, 13602, 2075, 2049, 7784, 1025, 2005, 2116, 1010, 2279, 4234, 2097, 2514, 2172, 2367, 1010, 1998, 2023, 5103, 2097, 2763, 2298, 2367, 1010, 2205, 1012, 2279, 4234, 2097, 2022, 1037, 2317, 4234, 1012, 1996, 15792, 10187, 3134, 2101, 2006, 1523, 5095, 2305, 2444, 1524, 2043, 3382, 1998, 6358, 2319, 5953, 2864, 1037, 11080, 2170, 1523, 29262, 13857, 1010, 1524, 2049, 4331, 4201, 5810, 1999, 2049, 4581, 1024, 1523, 2009, 1521, 1055, 1996, 2197, 4234, 2007, 13857, 2145, 2182, 1010, 1524, 1996, 18921, 7377, 3527, 9328, 3154, 1010, 1996, 6332, 1521, 1055, 11376, 25982, 1012, 2119, 3190, 12493, 1010, 3382, 2034, 2777, 8112, 2043, 2002, 2001, 1022, 2086, 2214, 1010, 2010, 2269, 1037, 3095, 2121, 2005, 1996, 8962, 2271, 2076, 2010, 4001, 2086, 1012, 2009, 3475, 1521, 1056, 2205, 2172, 1997, 1037, 7683, 2000, 6592, 2008, 3382, 1521, 1055, 2774, 2064, 2411, 7861, 23684, 1996, 3246, 8112, 16196, 2006, 1517, 4216, 1997, 7780, 2005, 2216, 22276, 2648, 1996, 3210, 1010, 1998, 2036, 1010, 2000, 11498, 8458, 23797, 29270, 1010, 2216, 2040, 2024, 6910, 1998, 8357, 2041, 1997, 2240, 1012, 3383, 2009, 1521, 1055, 2069, 1037, 16507, 2008, 3382, 2003, 2746, 2046, 2010, 2219, 2004, 1037, 7407, 8473, 1998, 3003, 2004, 8112, 18280, 2015, 1010, 2021, 2009, 3475, 1521, 1056, 1037, 16507, 2008, 2002, 4453, 2023, 4234, 2004, 1037, 4072, 2051, 2005, 8907, 1012, 2004, 2065, 9301, 1996, 10194, 1997, 1996, 2617, 1010, 2030, 2074, 2196, 2028, 2000, 3335, 2019, 4495, 2000, 2022, 12831, 1010, 3382, 1998, 3507, 3190, 2319, 15333, 28578, 19190, 4474, 2207, 1037, 6209, 8348, 2170, 12831, 4234, 13451, 1521, 9588, 1010, 1037, 4234, 2201, 2005, 1996, 2345, 9805, 7485, 5178, 1997, 1996, 8112, 3447, 1024, 2028, 2197, 26829, 1997, 3246, 1999, 2019, 9662, 1998, 25312, 18533, 2051, 1025, 1037, 12728, 4586, 7595, 2073, 4895, 10258, 2378, 8450, 12456, 6820, 2964, 1010, 18874, 1010, 1998, 2293, 1006, 2119, 11429, 2135, 1998, 18753, 1007, 3627, 1025, 1037, 3976, 3238, 5592, 12964, 1996, 2161, 1997, 3228, 1012, 1998, 2009, 3475, 1521, 1056, 2074, 2005, 2343, 8112, 1517, 2030, 2005, 13451, 1521, 9588, 1517, 2009, 1521, 1055, 2005, 2035, 1997, 2149, 1010, 2205, 1012, 2009, 1521, 1055, 8360, 4234, 2189, 2008, 1521, 1055, 5064, 2036, 2189, 102], [101, 10556, 25810, 14163, 28745, 22401, 2015, 4247, 2000, 3582, 2014, 8612, 1012, 1999, 2286, 1010, 2016, 2328, 2014, 5891, 2004, 2019, 3424, 23681, 2000, 2035, 2008, 2001, 2056, 2000, 2022, 9932, 2989, 2406, 2189, 1024, 1037, 3151, 2923, 1997, 27776, 19279, 7946, 14547, 1056, 16600, 2100, 7565, 1998, 1037, 23411, 2235, 1011, 2237, 3146, 4281, 1010, 2040, 2001, 2036, 18230, 2135, 6555, 1999, 2014, 2091, 1011, 2188, 22143, 1012, 2016, 11190, 21709, 2063, 9392, 12838, 1521, 1055, 12090, 2135, 2310, 15465, 3993, 7163, 1011, 17743, 1523, 9588, 1521, 1055, 3714, 2540, 1524, 1998, 2207, 2014, 2219, 2350, 1011, 3830, 2834, 1010, 2168, 9117, 2367, 2380, 1010, 2029, 3786, 2041, 4202, 9170, 1998, 1996, 6907, 1521, 1055, 25430, 27609, 2075, 10243, 2000, 2663, 1996, 2190, 2406, 2201, 8922, 1012, 1999, 2325, 1010, 2016, 2207, 12438, 3430, 1025, 2008, 2501, 2134, 1521, 1056, 2175, 2502, 1010, 2009, 2253, 2188, 1010, 9686, 5403, 9328, 2557, 1011, 5379, 4978, 2000, 3313, 2091, 9882, 2135, 2006, 1996, 7132, 1010, 12455, 2100, 16371, 6651, 1997, 2049, 8646, 1012, 2009, 2441, 2012, 2053, 1012, 1015, 2021, 2134, 1521, 1056, 5271, 2004, 2092, 3452, 1012, 2049, 3582, 1011, 2039, 1010, 1037, 2200, 10556, 25810, 4234, 1010, 2003, 2664, 2178, 2187, 2735, 2013, 14163, 28745, 22401, 2015, 1012, 1996, 2353, 2201, 2003, 3492, 2220, 1999, 2019, 3063, 1521, 1055, 12105, 2005, 1037, 6209, 2501, 1010, 2021, 2016, 11618, 2841, 2046, 2023, 2028, 2004, 2878, 27693, 2135, 2004, 2151, 5372, 6948, 1012, 2014, 2307, 4958, 11514, 4819, 2100, 2003, 1996, 2460, 3292, 2090, 10950, 29423, 1998, 13061, 3628, 1010, 2129, 2054, 2320, 2001, 2641, 26997, 2100, 1998, 7976, 2064, 1010, 2007, 2051, 1010, 2272, 2000, 4025, 16839, 9080, 12863, 1998, 2613, 1012, 14163, 28745, 22401, 2015, 1521, 2201, 24814, 2039, 1996, 3054, 1011, 1521, 20341, 3690, 26968, 1997, 1037, 4918, 2829, 4234, 1010, 20292, 8100, 2013, 2014, 2511, 2530, 1011, 7370, 5466, 5963, 12465, 2000, 18628, 11714, 12564, 2050, 1998, 13528, 3769, 1010, 2007, 2019, 6739, 2135, 17940, 2299, 4989, 2008, 12671, 2006, 3409, 2100, 3117, 7368, 1010, 2465, 2100, 4781, 1010, 1998, 1037, 4518, 2075, 1521, 1055, 4276, 1997, 23728, 1012, 2138, 2256, 4234, 5633, 8632, 2039, 2058, 1996, 2086, 1010, 2000, 2022, 6497, 2098, 2125, 2007, 1996, 2060, 14529, 2005, 1037, 2261, 3134, 1998, 2059, 2404, 2067, 1999, 2037, 8378, 1010, 2027, 2089, 2022, 2028, 1997, 1996, 2261, 4127, 1997, 4042, 2116, 2111, 2145, 2377, 1999, 2440, 1012, 1996, 24558, 1997, 1037, 2200, 10556, 25810, 4234, 20397, 2023, 5056, 1012, 14163, 28745, 22401, 2015, 2987, 1521, 1056, 5481, 2014, 9530, 3401, 4183, 1010, 3098, 11552, 2135, 2021, 7511, 2135, 2007, 10145, 1998, 15749, 3886, 2006, 1523, 2031, 4426, 1037, 12831, 2210, 4234, 1010, 1524, 2628, 2011, 1037, 22889, 7416, 5603, 10118, 1011, 1998, 15888, 1011, 9669, 2098, 1523, 2292, 2009, 4586, 1524, 2007, 3507, 2530, 7370, 2545, 1996, 10861, 4783, 5208, 1012, 2016, 3065, 2014, 2192, 2279, 2006, 1517, 1997, 2035, 2477, 999, 1517, 1037, 29499, 1011, 2066, 3104, 1997, 1523, 4234, 2123, 1521, 1056, 2022, 2397, 1010, 1524, 1037, 1013, 1047, 1013, 1037, 1523, 1996, 9090, 23041, 102], [101, 2129, 2079, 2017, 2754, 2019, 3850, 2008, 3262, 3138, 3182, 2503, 1037, 2775, 1521, 1055, 25303, 2568, 1029, 2008, 2001, 2028, 3291, 5307, 4543, 3841, 10097, 1998, 21091, 6916, 3367, 2585, 13433, 16671, 5420, 2043, 2027, 2275, 2041, 2000, 2191, 1037, 3315, 4258, 3538, 1517, 10097, 1521, 1055, 2034, 2412, 1517, 1997, 24486, 5085, 1521, 3118, 8317, 5469, 3117, 1010, 1996, 19411, 4713, 1012, 1996, 2338, 4136, 1996, 6925, 1997, 1037, 18224, 25940, 9458, 2315, 3581, 1010, 2040, 14386, 8583, 13576, 1010, 6355, 13549, 2006, 1037, 4078, 19425, 4104, 2479, 1012, 2178, 3291, 1024, 3581, 2003, 1037, 28616, 15707, 26942, 1010, 1998, 2023, 2003, 2053, 2051, 2005, 1996, 1043, 10626, 9031, 1997, 28616, 15707, 26942, 2594, 3424, 5886, 22504, 1012, 10097, 13332, 2119, 3471, 1999, 2028, 6909, 2011, 4851, 13433, 16671, 5420, 2000, 3975, 3581, 1521, 1055, 6887, 26802, 26212, 20255, 2594, 18847, 24277, 2046, 1037, 3979, 8649, 5657, 1012, 2059, 2002, 4137, 1996, 2093, 4395, 2000, 3928, 2308, 1010, 4498, 18168, 12474, 2075, 3581, 2004, 1037, 2839, 1012, 2006, 2023, 4397, 1011, 2207, 5010, 2451, 3405, 1517, 1996, 3850, 5885, 1999, 2286, 1517, 2027, 2024, 7042, 2007, 10433, 25869, 2964, 2050, 2011, 3658, 18349, 2102, 2139, 18575, 1010, 8183, 17080, 2015, 20105, 1010, 1998, 3814, 2213, 2813, 4765, 2378, 1012, 2025, 6414, 5919, 1997, 3581, 1521, 1055, 25774, 1010, 2027, 1521, 2128, 2066, 1996, 6519, 3111, 1010, 20905, 2015, 1997, 7746, 25928, 1012, 2007, 15123, 10768, 21735, 1010, 2027, 21271, 1998, 13183, 3581, 1521, 1055, 1999, 7512, 12032, 12633, 1010, 1998, 10097, 1521, 1055, 2189, 1010, 2209, 2011, 1996, 28559, 8254, 14876, 6200, 1010, 24745, 18716, 1996, 7311, 1012, 1996, 2660, 1011, 2141, 1010, 10399, 1011, 2241, 10097, 2003, 16071, 2124, 2005, 2010, 16175, 1011, 6490, 23376, 1997, 4556, 2189, 1998, 3082, 3384, 1012, 2002, 13585, 2695, 1011, 10124, 21892, 3366, 2007, 19770, 18976, 1998, 10304, 2937, 15236, 1010, 13003, 2046, 1996, 17578, 22159, 2229, 1997, 4251, 1998, 4040, 2075, 2039, 1996, 10882, 28077, 2102, 11373, 1997, 5005, 1012, 2295, 1996, 3579, 2003, 2085, 2444, 2614, 1998, 1996, 2529, 2376, 1010, 1996, 19411, 4713, 2323, 2145, 24501, 21149, 2007, 4599, 1997, 2010, 17093, 2189, 1012, 2009, 1521, 1055, 2011, 1996, 3759, 20037, 1010, 18921, 16365, 2075, 4556, 1998, 28290, 2075, 8139, 1010, 2007, 2053, 23077, 1999, 10097, 1521, 1055, 8085, 12586, 1997, 9530, 7874, 12742, 11655, 7962, 1010, 10059, 8153, 1010, 1998, 17796, 7961, 1012, 1037, 2235, 5164, 7241, 23894, 1999, 4389, 21708, 2015, 1010, 18296, 2075, 2041, 5903, 8649, 6444, 2015, 1999, 1037, 7570, 21197, 20721, 5396, 1997, 3321, 1998, 20870, 1012, 2004, 2467, 1010, 10097, 1521, 1055, 23760, 22852, 6490, 2389, 10466, 2123, 1521, 1056, 4025, 12613, 1517, 2027, 2074, 4895, 9336, 3490, 2135, 4839, 1010, 13366, 2953, 6562, 2686, 7292, 1012, 2023, 2003, 4110, 1999, 1037, 3405, 1997, 9680, 2102, 1010, 20014, 7946, 3512, 28398, 1010, 1996, 8453, 11192, 2075, 2046, 2115, 2227, 1010, 4406, 1996, 13205, 6366, 5171, 1997, 4556, 2189, 1012, 2009, 1521, 1055, 2025, 1996, 2069, 2126, 1996, 19411, 4713, 14189, 22534, 4680, 1012, 2123, 1521, 1056, 5987, 25302, 6887, 8180, 2075, 2030, 102], [101, 1996, 3016, 2181, 3384, 2166, 2869, 2040, 15821, 15485, 2256, 16429, 14604, 2015, 2024, 14258, 3625, 2005, 2070, 1997, 1996, 26858, 2189, 1999, 1996, 5230, 1010, 2021, 2362, 2027, 2202, 1037, 11259, 2099, 1010, 12430, 3921, 1012, 8053, 17793, 2000, 1521, 16002, 4490, 2066, 2023, 9801, 17085, 2030, 2522, 6593, 10207, 8178, 2004, 2037, 16682, 2006, 1996, 13769, 19544, 15738, 1010, 15485, 2256, 16429, 14604, 2015, 21490, 1037, 24407, 1010, 25976, 2100, 7224, 1012, 2144, 24469, 1999, 2289, 1010, 2027, 2031, 17824, 2135, 20037, 1996, 3151, 3252, 1997, 12677, 3384, 1998, 2695, 1011, 2600, 1006, 4895, 16570, 26951, 2135, 3082, 4042, 2007, 17093, 25347, 2015, 1007, 1010, 2437, 2189, 2008, 21096, 2015, 19080, 1998, 5168, 1010, 20728, 2069, 2043, 1996, 6980, 2064, 2053, 2936, 2022, 4838, 1012, 2066, 2272, 1996, 22794, 2860, 1010, 2037, 6581, 2262, 2201, 1998, 2034, 2007, 12943, 8095, 11663, 1013, 1058, 14854, 7101, 29347, 28793, 2139, 23793, 1010, 2037, 3582, 1011, 2039, 2054, 4519, 3238, 6440, 2003, 2069, 2416, 3162, 2146, 1010, 2295, 2009, 14798, 2058, 2019, 3178, 1012, 4129, 2135, 1010, 2009, 3138, 15485, 2256, 16429, 14604, 2015, 2321, 2781, 2046, 1996, 2201, 1998, 8576, 2083, 1996, 2117, 2650, 1010, 1523, 3714, 5750, 1010, 1524, 2077, 2027, 4895, 19738, 4095, 1996, 2440, 2486, 1997, 2037, 2614, 1012, 2127, 2059, 1010, 2009, 1521, 1055, 3262, 3990, 2955, 2058, 8201, 2126, 1521, 1055, 24222, 7334, 1998, 13895, 9350, 2232, 1521, 1055, 18036, 22658, 3321, 12735, 1010, 2007, 2139, 23793, 1521, 1055, 2396, 3993, 22980, 10591, 25212, 11272, 2114, 1996, 3681, 1012, 2043, 1523, 3714, 5750, 1524, 6561, 2049, 14463, 1010, 2027, 2035, 28314, 2046, 1037, 8505, 3560, 11950, 1012, 9350, 2232, 1998, 2126, 10236, 2037, 2955, 2105, 2169, 2060, 2066, 1037, 5164, 2930, 1010, 23990, 2058, 2126, 1521, 1055, 14527, 24808, 2015, 1025, 2362, 2037, 5755, 2433, 1996, 14669, 2422, 1997, 1996, 2201, 1012, 2021, 2096, 15485, 2256, 16429, 14604, 2015, 2031, 2196, 5015, 2062, 3928, 2004, 1037, 2093, 1011, 3538, 1010, 2122, 2828, 1997, 13675, 2229, 23865, 2891, 2024, 2025, 1996, 3579, 1012, 1996, 2087, 14527, 11373, 2612, 2272, 2013, 2049, 19013, 5312, 1010, 2066, 1996, 27724, 2075, 19429, 2050, 2000, 1523, 1006, 2001, 2009, 1007, 1996, 10311, 4355, 2518, 1524, 2030, 1996, 20161, 17174, 1997, 1523, 10557, 1997, 5192, 1012, 1524, 2802, 1996, 2501, 1010, 1996, 7146, 9319, 1037, 8335, 1010, 3011, 2100, 6888, 1010, 5681, 2652, 2125, 2028, 2178, 2007, 1996, 26406, 1997, 2019, 17727, 12298, 2177, 1012, 2009, 13956, 4010, 2256, 16429, 14604, 2015, 2000, 2047, 7535, 1012, 2126, 1521, 1055, 2858, 2064, 7065, 2121, 5677, 3686, 2007, 1037, 9479, 1010, 2439, 3307, 22912, 1998, 26831, 2007, 1037, 12392, 11895, 15810, 1010, 2411, 18435, 1996, 2201, 2049, 14902, 1025, 9350, 2232, 1521, 1055, 3321, 2652, 4240, 2004, 2049, 17187, 3192, 1012, 2139, 23793, 1010, 5564, 1010, 9686, 5403, 9333, 2010, 5171, 6101, 2007, 1037, 2062, 8790, 27396, 1012, 2010, 4030, 6986, 3334, 1999, 1996, 3054, 29015, 1997, 1523, 6114, 3392, 1524, 3957, 2009, 1996, 3110, 1997, 2019, 27660, 4234, 8594, 1010, 2096, 2010, 6918, 1010, 3313, 3321, 6943, 9372, 1999, 1523, 1006, 2001, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0oXLMLF0P9Az"
   },
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx.tolist()\n",
    "    item = {key: torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "data_train = customDataset(x_train, y_train)\n",
    "data_test = customDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174,
     "referenced_widgets": [
      "89e67e23cab346088fbc5f8bad9e0a26",
      "7a895ccee3e94ac591e8f1bbaacf79ff",
      "18154712e27d4a52857235032aad2809",
      "110e89a85c324de08bdcce38ff929c66",
      "158693a626d94eb0bb02e8c6cbfabb4d",
      "7cf9d4fcf1c34b3abb133b7f2bcfbba1",
      "fa0bd7386c364017a288da7c731d8774",
      "b6d80261d0cc4d0ba7000fb55f003908"
     ]
    },
    "id": "oigbzkcqNVPU",
    "outputId": "bea790a9-04cc-4afa-98f9-688f9b53b53d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e67e23cab346088fbc5f8bad9e0a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"./train_models\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = batch_size\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = data_train,\n",
    "    eval_dataset = data_test,\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "JCdZWMqO3TtE",
    "outputId": "9cd0bba9-9bfb-418e-b90a-3508771d1e1d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2001' max='2300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2001/2300 31:59 < 04:47, 1.04 it/s, Epoch 4.35/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.381325</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>119.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.374561</td>\n",
       "      <td>30.760600</td>\n",
       "      <td>119.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>30.765300</td>\n",
       "      <td>119.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.448878</td>\n",
       "      <td>30.760400</td>\n",
       "      <td>119.472000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2300' max='2300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2300/2300 37:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.381325</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>119.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.374561</td>\n",
       "      <td>30.760600</td>\n",
       "      <td>119.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>30.765300</td>\n",
       "      <td>119.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.448878</td>\n",
       "      <td>30.760400</td>\n",
       "      <td>119.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.505378</td>\n",
       "      <td>30.754200</td>\n",
       "      <td>119.496000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2300, training_loss=0.27035701109015425, metrics={'train_runtime': 2221.6749, 'train_samples_per_second': 1.035, 'total_flos': 1.51148203310592e+16, 'epoch': 5.0, 'init_mem_cpu_alloc_delta': 1875173376, 'init_mem_gpu_alloc_delta': 268953088, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 111755264, 'train_mem_gpu_alloc_delta': 816905728, 'train_mem_cpu_peaked_delta': 1867776, 'train_mem_gpu_peaked_delta': 13072364544})"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "ya2RX1PO0t8S",
    "outputId": "70b499b6-4501-4f3a-809f-6c2ade02949a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGB8Hm9S1-RA",
    "outputId": "275f744a-d2e7-4022-e6ca-f03a7736be04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2979,    0],\n",
       "       [   0,  696]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "confusion_matrix(predictions[1],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj8bfN9017Th",
    "outputId": "281e04e3-e1e0-4ee5-e40c-4d43e1f9a7a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.35213690996170044,\n",
       " 'test_mem_cpu_alloc_delta': 0,\n",
       " 'test_mem_cpu_peaked_delta': 0,\n",
       " 'test_mem_gpu_alloc_delta': 0,\n",
       " 'test_mem_gpu_peaked_delta': 79995904,\n",
       " 'test_runtime': 0.2568,\n",
       " 'test_samples_per_second': 778.836}"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ketchupredictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvINUbX20-aN",
    "outputId": "bfbb1e26-56e2-4d95-9ced-57b3a78838da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "wnmgYOXQ0e08",
    "outputId": "c268ce62-a0cd-4249-b6e5-5a830ec6c6ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 10.0,\n",
       " 'eval_loss': 0.35213690996170044,\n",
       " 'eval_mem_cpu_alloc_delta': 4096,\n",
       " 'eval_mem_cpu_peaked_delta': 0,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 79992832,\n",
       " 'eval_runtime': 0.1574,\n",
       " 'eval_samples_per_second': 1270.961}"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Etgf9Jurzxnz",
    "outputId": "3b7db4b8-e6cb-40b3-eb55-0714a06433a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Baseline Sentiment Analysis.ipynb'   PitchforkReviewAnalysis.ipynb\n",
      " CurrentPF.ipynb\t\t      Pitchfork-Reviews\n",
      " Data\t\t\t\t      README.md\n",
      "'Data Loading.ipynb'\t\t      Reports\n",
      " EDA.ipynb\t\t\t      runs\n",
      " logs\t\t\t\t      train_models\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('train_models').to('cpu')\n",
    "trainer = Trainer(\n",
    "  model,\n",
    "  TrainingArguments(\"poop\"),\n",
    "  train_dataset = data_train,\n",
    "  eval_dataset = data_test,\n",
    "  tokenizer = tokenizer\n",
    "  )\n",
    "outputs = model(torch.tensor(x_test['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge3sy-BaIKCK"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "def fit(model, trainDataset, valDataset, epochs = 1, batchSize = 32, lr = 1e-3, scheduler=None):\n",
    "  device = xm.xla_device()\n",
    "  model = model.to(device)\n",
    "  \n",
    "  trainSampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    trainDataset,\n",
    "    num_replicas=xm.xrt_world_size(),\n",
    "    rank=xm.get_ordinal(),\n",
    "    shuffle=True)\n",
    "  valSampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    valDataset,\n",
    "    num_replicas=xm.xrt_world_size(),\n",
    "    rank=xm.get_ordinal(),\n",
    "    shuffle=False)\n",
    "  trainLoader = DataLoader(trainDataset, \n",
    "                           batch_size=batchSize, \n",
    "                           sampler=trainSampler, \n",
    "                           num_workers=0, \n",
    "                           drop_last = True)\n",
    "  valLoader = DataLoader(valDataset, \n",
    "                         batch_size=batchSize, \n",
    "                         shuffle = False, \n",
    "                         sampler = valSampler,\n",
    "                         num_workers=0,\n",
    "                         drop_last=False)\n",
    "  num_train_steps = int(len(trainDataset) / batchSize /xm.xrt_world_size()*epochs)\n",
    "  optimizer = AdamW(model.parameters(), lr = lr)\n",
    "  # scheduler = get_linear_schedule_with_warmup(\n",
    "  #     optimizer, num_warmup_steps = 0, num_training_steps = num_train_steps\n",
    "  # )\n",
    "  # scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "  #     optimizer, step_size = 2, gamma = 0.1\n",
    "  # )\n",
    "  # scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "  #     optimizer,\n",
    "  #     base_lr = 5e-5, max_lr = lr,\n",
    "  #     mode = 'triangular2',\n",
    "  #     step_size_up = int(len(trainDataset)/32/xm.xrt_world_size())*3, #3 being the number of epochs to go up.\n",
    "  #     cycle_momentum = False\n",
    "  # )\n",
    "  def train_loop(trainLoader):\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    for batch in trainLoader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "      loss = outputs[0]\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer, barrier = True)\n",
    "      if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    print(f'Final Training Loss: {loss}')\n",
    "  def eval_loop(valLoader):\n",
    "    with torch.no_grad():\n",
    "      for batch in valLoader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "      print(f'Final Val Loss: {loss}')\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    para_loader = pl.ParallelLoader(trainLoader,[device])\n",
    "    train_loop(para_loader.per_device_loader(device))\n",
    "    # _lr = scheduler.get_last_lr()[0]\n",
    "    # print(_lr)\n",
    "    print(f'epoch number: {epoch}')\n",
    "    del para_loader\n",
    "    \n",
    "    model.eval()\n",
    "    para_loader = pl.ParallelLoader(valLoader, [device])\n",
    "    eval_loop(para_loader.per_device_loader(device))\n",
    "    del para_loader\n",
    "  xm.save(model.state_dict(), \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVIK0lZEHn6s",
    "outputId": "56f5d210-ea80-46a2-f0f7-689f887a8044"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss: 1.3492283821105957\n",
      "epoch number: 0\n",
      "Final Training Loss: 1.2990268468856812\n",
      "Final Training Loss: 1.7547035217285156\n",
      "Final Training Loss: 1.068077802658081\n",
      "Final Training Loss: 1.7174874544143677\n",
      "epoch number: 0\n",
      "epoch number: 0\n",
      "Final Training Loss: 1.5977767705917358\n",
      "epoch number: 0\n",
      "Final Training Loss: 1.4827221632003784\n",
      "Final Training Loss: 1.4326668977737427\n",
      "epoch number: 0\n",
      "epoch number: 0\n",
      "epoch number: 0\n",
      "epoch number: 0\n",
      "Final Val Loss: 1.555977463722229\n",
      "Final Val Loss: 1.5560131072998047\n",
      "Final Val Loss: 1.440492868423462\n",
      "Final Val Loss: 1.2094310522079468\n",
      "Final Val Loss: 2.0180861949920654\n",
      "Final Val Loss: 1.440464735031128\n",
      "Final Val Loss: 1.9026119709014893\n",
      "Final Val Loss: 2.133604049682617\n",
      "Final Training Loss: 0.7239044308662415\n",
      "Final Training Loss: 0.7833437919616699\n",
      "epoch number: 1\n",
      "Final Training Loss: 0.6569998860359192\n",
      "Final Training Loss: 0.7507330179214478\n",
      "epoch number: 1\n",
      "Final Training Loss: 0.6701981425285339\n",
      "Final Training Loss: 0.7880423069000244\n",
      "Final Training Loss: 0.7569464445114136\n",
      "epoch number: 1\n",
      "epoch number: 1\n",
      "epoch number: 1\n",
      "epoch number: 1\n",
      "epoch number: 1\n",
      "Final Training Loss: 0.6877008676528931\n",
      "epoch number: 1\n",
      "Final Val Loss: 0.808489203453064\n",
      "Final Val Loss: 0.7420382499694824\n",
      "Final Val Loss: 0.7198901772499084\n",
      "Final Val Loss: 0.6755872368812561\n",
      "Final Val Loss: 0.8527928590774536\n",
      "Final Val Loss: 0.7198887467384338\n",
      "Final Val Loss: 0.8306413888931274\n",
      "Final Val Loss: 0.7420399785041809\n",
      "Final Training Loss: 0.6900995373725891\n",
      "Final Training Loss: 0.6534481048583984\n",
      "epoch number: 2\n",
      "Final Training Loss: 0.7453155517578125\n",
      "Final Training Loss: 0.7016697525978088\n",
      "Final Training Loss: 0.6835172176361084\n",
      "epoch number: 2\n",
      "Final Training Loss: 0.6963008642196655\n",
      "Final Training Loss: 0.7330857515335083\n",
      "epoch number: 2\n",
      "epoch number: 2\n",
      "Final Training Loss: 0.7155909538269043\n",
      "epoch number: 2\n",
      "epoch number: 2\n",
      "epoch number: 2\n",
      "epoch number: 2\n",
      "Final Val Loss: 0.7749083638191223\n",
      "Final Val Loss: 0.7168290615081787\n",
      "Final Val Loss: 0.7168292999267578\n",
      "Final Val Loss: 0.7023090124130249\n",
      "Final Val Loss: 0.6732689738273621\n",
      "Final Val Loss: 0.7603888511657715\n",
      "Final Val Loss: 0.7894287705421448\n",
      "Final Val Loss: 0.7023090124130249\n",
      "Final Training Loss: 0.6925545930862427\n",
      "epoch number: 3\n",
      "Final Training Loss: 0.6931586265563965\n",
      "Final Training Loss: 0.6916486024856567\n",
      "Final Training Loss: 0.6928566694259644\n",
      "Final Training Loss: 0.6937627792358398\n",
      "Final Training Loss: 0.6928566098213196\n",
      "epoch number: 3\n",
      "Final Training Loss: 0.6922526359558105\n",
      "epoch number: 3\n",
      "Final Training Loss: 0.6937626600265503\n",
      "epoch number: 3\n",
      "epoch number: 3\n",
      "epoch number: 3\n",
      "epoch number: 3\n",
      "epoch number: 3\n",
      "Final Val Loss: 0.6947826147079468\n",
      "Final Val Loss: 0.6922540664672852\n",
      "Final Val Loss: 0.6933377385139465\n",
      "Final Val Loss: 0.6929765939712524\n",
      "Final Val Loss: 0.6951438784599304\n",
      "Final Val Loss: 0.6933377385139465\n",
      "Final Val Loss: 0.6944214701652527\n",
      "Final Val Loss: 0.6929765939712524\n",
      "Final Training Loss: 0.6934604644775391\n",
      "Final Training Loss: 0.6936688423156738\n",
      "Final Training Loss: 0.6929396390914917\n",
      "Final Training Loss: 0.6929395794868469\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "Final Training Loss: 0.6932522058486938\n",
      "Final Training Loss: 0.6931480169296265\n",
      "Final Training Loss: 0.6933563351631165\n",
      "Final Training Loss: 0.6932521462440491\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "epoch number: 4\n",
      "Final Val Loss: 0.6930768489837646\n",
      "Final Val Loss: 0.6932195425033569\n",
      "Final Val Loss: 0.692363440990448\n",
      "Final Val Loss: 0.6930768489837646\n",
      "Final Val Loss: 0.6926487684249878\n",
      "Final Val Loss: 0.6935049295425415\n",
      "Final Val Loss: 0.6932195425033569\n",
      "Final Val Loss: 0.6925060749053955\n",
      "Final Training Loss: 0.6942113637924194\n",
      "Final Training Loss: 0.6924632787704468\n",
      "Final Training Loss: 0.6931624412536621\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "Final Training Loss: 0.6949105262756348\n",
      "Final Training Loss: 0.6935120820999146\n",
      "Final Training Loss: 0.6924631595611572\n",
      "Final Training Loss: 0.6935120224952698\n",
      "Final Training Loss: 0.693861722946167\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "epoch number: 5\n",
      "Final Val Loss: 0.6906630396842957\n",
      "Final Val Loss: 0.693389892578125\n",
      "Final Val Loss: 0.6942987442016602\n",
      "Final Val Loss: 0.6929354071617126\n",
      "Final Val Loss: 0.6933897733688354\n",
      "Final Val Loss: 0.6911176443099976\n",
      "Final Val Loss: 0.6915719509124756\n",
      "Final Val Loss: 0.6929353475570679\n",
      "Final Training Loss: 0.6920944452285767\n",
      "Final Training Loss: 0.6942853331565857\n",
      "Final Training Loss: 0.6937292814254761\n",
      "Final Training Loss: 0.6937376260757446\n",
      "Final Training Loss: 0.6948331594467163\n",
      "Final Training Loss: 0.6931816339492798\n",
      "Final Training Loss: 0.6920944452285767\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "Final Training Loss: 0.6959285736083984\n",
      "epoch number: 6\n",
      "epoch number: 6\n",
      "Final Val Loss: 0.6935392022132874\n",
      "Final Val Loss: 0.6900095343589783\n",
      "Final Val Loss: 0.6935392022132874\n",
      "Final Val Loss: 0.6893035769462585\n",
      "Final Val Loss: 0.6928332448005676\n",
      "Final Val Loss: 0.6949509978294373\n",
      "Final Val Loss: 0.690715491771698\n",
      "Final Val Loss: 0.6928332448005676\n",
      "Final Training Loss: 0.6918045878410339\n",
      "Final Training Loss: 0.6946149468421936\n",
      "Final Training Loss: 0.696722686290741\n",
      "Final Training Loss: 0.6939123868942261\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "Final Training Loss: 0.6939123868942261\n",
      "Final Training Loss: 0.6932097673416138\n",
      "Final Training Loss: 0.6918045282363892\n",
      "Final Training Loss: 0.6953175067901611\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "epoch number: 7\n",
      "Final Val Loss: 0.6927589774131775\n",
      "Final Val Loss: 0.6927589774131775\n",
      "Final Val Loss: 0.6891502141952515\n",
      "Final Val Loss: 0.6900524497032166\n",
      "Final Val Loss: 0.6882480978965759\n",
      "Final Val Loss: 0.6936611533164978\n",
      "Final Val Loss: 0.6936611533164978\n",
      "Final Val Loss: 0.6954655051231384\n",
      "Final Training Loss: 0.6932323575019836\n",
      "Final Training Loss: 0.6948695182800293\n",
      "Final Training Loss: 0.691595196723938\n",
      "Final Training Loss: 0.6956881284713745\n",
      "Final Training Loss: 0.6973254084587097\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "Final Training Loss: 0.6940509676933289\n",
      "Final Training Loss: 0.6940509676933289\n",
      "Final Training Loss: 0.6915951371192932\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "epoch number: 8\n",
      "Final Val Loss: 0.6874634623527527\n",
      "Final Val Loss: 0.6885123252868652\n",
      "Final Val Loss: 0.6927081346511841\n",
      "Final Val Loss: 0.6927081346511841\n",
      "Final Val Loss: 0.6958548426628113\n",
      "Final Val Loss: 0.6937569975852966\n",
      "Final Val Loss: 0.6895613074302673\n",
      "Final Val Loss: 0.6937569975852966\n",
      "Final Training Loss: 0.6950549483299255\n",
      "Final Training Loss: 0.6932510137557983\n",
      "Final Training Loss: 0.697760820388794\n",
      "Final Training Loss: 0.6914470195770264\n",
      "Final Training Loss: 0.6959568858146667\n",
      "Final Training Loss: 0.6941529512405396\n",
      "Final Training Loss: 0.6914470791816711\n",
      "Final Training Loss: 0.6941530108451843\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "epoch number: 9\n",
      "Final Val Loss: 0.6869027614593506\n",
      "Final Val Loss: 0.6926736831665039\n",
      "Final Val Loss: 0.6926738023757935\n",
      "Final Val Loss: 0.6892111301422119\n",
      "Final Val Loss: 0.6938279271125793\n",
      "Final Val Loss: 0.688057005405426\n",
      "Final Val Loss: 0.6938279271125793\n",
      "Final Val Loss: 0.6961363554000854\n",
      "Final Training Loss: 0.69422447681427\n",
      "Final Training Loss: 0.6980623602867126\n",
      "Final Training Loss: 0.6913461089134216\n",
      "Final Training Loss: 0.6913461089134216\n",
      "Final Training Loss: 0.6961434483528137\n",
      "Final Training Loss: 0.6951839923858643\n",
      "Final Training Loss: 0.6942245364189148\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "epoch number: 10\n",
      "Final Training Loss: 0.6932650208473206\n",
      "epoch number: 10\n",
      "Final Val Loss: 0.6938780546188354\n",
      "Final Val Loss: 0.6889716386795044\n",
      "Final Val Loss: 0.6865183711051941\n",
      "Final Val Loss: 0.6926514506340027\n",
      "Final Val Loss: 0.6938780546188354\n",
      "Final Val Loss: 0.6926514506340027\n",
      "Final Val Loss: 0.696331262588501\n",
      "Final Val Loss: 0.6877449750900269\n",
      "Final Training Loss: 0.6962662935256958\n",
      "Final Training Loss: 0.6942710876464844\n",
      "Final Training Loss: 0.6912782788276672\n",
      "Final Training Loss: 0.6952686905860901\n",
      "Final Training Loss: 0.6932734847068787\n",
      "Final Training Loss: 0.6942710876464844\n",
      "Final Training Loss: 0.6982614994049072\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "Final Training Loss: 0.6912782788276672\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "epoch number: 11\n",
      "Final Val Loss: 0.6939103007316589\n",
      "Final Val Loss: 0.6862630844116211\n",
      "Final Val Loss: 0.687537670135498\n",
      "Final Val Loss: 0.6939101815223694\n",
      "Final Val Loss: 0.6964592337608337\n",
      "Final Val Loss: 0.6888121366500854\n",
      "Final Val Loss: 0.692635715007782\n",
      "Final Val Loss: 0.692635715007782\n",
      "Final Training Loss: 0.6943016052246094\n",
      "Final Training Loss: 0.6983894109725952\n",
      "Final Training Loss: 0.6912357807159424\n",
      "Final Training Loss: 0.6932796835899353\n",
      "Final Training Loss: 0.6963454484939575\n",
      "Final Training Loss: 0.6912357807159424\n",
      "epoch number: 12\n",
      "Final Training Loss: 0.6953235864639282\n",
      "epoch number: 12\n",
      "Final Training Loss: 0.6943016648292542\n",
      "epoch number: 12\n",
      "epoch number: 12\n",
      "epoch number: 12\n",
      "epoch number: 12\n",
      "epoch number: 12\n",
      "epoch number: 12\n",
      "Final Val Loss: 0.6926265358924866\n",
      "Final Val Loss: 0.6861017346382141\n",
      "Final Val Loss: 0.6939315795898438\n",
      "Final Val Loss: 0.6874067783355713\n",
      "Final Val Loss: 0.6939315795898438\n",
      "Final Val Loss: 0.6887117028236389\n",
      "Final Val Loss: 0.6926265358924866\n",
      "Final Val Loss: 0.696541428565979\n",
      "Final Training Loss: 0.696394145488739\n",
      "Final Training Loss: 0.6943203806877136\n",
      "Final Training Loss: 0.6984679698944092\n",
      "Final Training Loss: 0.6943203806877136\n",
      "Final Training Loss: 0.6953572630882263\n",
      "Final Training Loss: 0.6932834386825562\n",
      "Final Training Loss: 0.691209614276886\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "Final Training Loss: 0.6912096738815308\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "epoch number: 13\n",
      "Final Val Loss: 0.6860028505325317\n",
      "Final Val Loss: 0.6926209926605225\n",
      "Final Val Loss: 0.69394451379776\n",
      "Final Val Loss: 0.6939443945884705\n",
      "Final Val Loss: 0.6873264908790588\n",
      "Final Val Loss: 0.6965917944908142\n",
      "Final Val Loss: 0.6926208138465881\n",
      "Final Val Loss: 0.6886501312255859\n",
      "Final Training Loss: 0.6964234709739685\n",
      "Final Training Loss: 0.6911941170692444\n",
      "Final Training Loss: 0.6943316459655762\n",
      "Final Training Loss: 0.6911941766738892\n",
      "epoch number: 14\n",
      "Final Training Loss: 0.6985151171684265\n",
      "epoch number: 14\n",
      "Final Training Loss: 0.69537752866745\n",
      "Final Training Loss: 0.6943316459655762\n",
      "epoch number: 14\n",
      "Final Training Loss: 0.6932858824729919\n",
      "epoch number: 14\n",
      "epoch number: 14\n",
      "epoch number: 14\n",
      "epoch number: 14\n",
      "epoch number: 14\n",
      "Final Val Loss: 0.6859445571899414\n",
      "Final Val Loss: 0.6966215968132019\n",
      "Final Val Loss: 0.6939523220062256\n",
      "Final Val Loss: 0.6939523220062256\n",
      "Final Val Loss: 0.6926177144050598\n",
      "Final Val Loss: 0.6872791051864624\n",
      "Final Val Loss: 0.688613772392273\n",
      "Final Val Loss: 0.6926177740097046\n",
      "Final Training Loss: 0.6943382024765015\n",
      "Final Training Loss: 0.698542594909668\n",
      "Final Training Loss: 0.6953892707824707\n",
      "Final Training Loss: 0.6964403986930847\n",
      "Final Training Loss: 0.6911849975585938\n",
      "Final Training Loss: 0.6932871341705322\n",
      "Final Training Loss: 0.6943382024765015\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "Final Training Loss: 0.6911849975585938\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "epoch number: 15\n",
      "Final Val Loss: 0.6859109401702881\n",
      "Final Val Loss: 0.6939567923545837\n",
      "Final Val Loss: 0.692615807056427\n",
      "Final Val Loss: 0.6966388821601868\n",
      "Final Val Loss: 0.6885929107666016\n",
      "Final Val Loss: 0.6939567923545837\n",
      "Final Val Loss: 0.6872518658638\n",
      "Final Val Loss: 0.6926158666610718\n",
      "Final Training Loss: 0.6911797523498535\n",
      "Final Training Loss: 0.6985586881637573\n",
      "Final Training Loss: 0.6911796927452087\n",
      "Final Training Loss: 0.6953963041305542\n",
      "Final Training Loss: 0.6943421363830566\n",
      "Final Training Loss: 0.696450412273407\n",
      "Final Training Loss: 0.6943421959877014\n",
      "Final Training Loss: 0.6932879686355591\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "epoch number: 16\n",
      "Final Val Loss: 0.6939594149589539\n",
      "Final Val Loss: 0.6926147937774658\n",
      "Final Val Loss: 0.6885807514190674\n",
      "Final Val Loss: 0.6966487765312195\n",
      "Final Val Loss: 0.6872362494468689\n",
      "Final Val Loss: 0.692614734172821\n",
      "Final Val Loss: 0.6939594149589539\n",
      "Final Val Loss: 0.6858915686607361\n",
      "Final Training Loss: 0.6964566111564636\n",
      "Final Training Loss: 0.695400595664978\n",
      "Final Training Loss: 0.694344699382782\n",
      "Final Training Loss: 0.6943445801734924\n",
      "Final Training Loss: 0.6911765933036804\n",
      "Final Training Loss: 0.6985687017440796\n",
      "epoch number: 17\n",
      "Final Training Loss: 0.6932886242866516\n",
      "epoch number: 17\n",
      "epoch number: 17\n",
      "epoch number: 17\n",
      "epoch number: 17\n",
      "epoch number: 17\n",
      "epoch number: 17\n",
      "Final Training Loss: 0.6911765933036804\n",
      "epoch number: 17\n",
      "Final Val Loss: 0.6885737180709839\n",
      "Final Val Loss: 0.6858801245689392\n",
      "Final Val Loss: 0.6926140189170837\n",
      "Final Val Loss: 0.6939607858657837\n",
      "Final Val Loss: 0.6939607858657837\n",
      "Final Val Loss: 0.6926140189170837\n",
      "Final Val Loss: 0.6966543197631836\n",
      "Final Val Loss: 0.6872268319129944\n",
      "Final Training Loss: 0.6964606046676636\n",
      "Final Training Loss: 0.6954033374786377\n",
      "Final Training Loss: 0.691174328327179\n",
      "Final Training Loss: 0.691174328327179\n",
      "Final Training Loss: 0.6943460702896118\n",
      "Final Training Loss: 0.6943461298942566\n",
      "Final Training Loss: 0.6985751390457153\n",
      "Final Training Loss: 0.6932888627052307\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "epoch number: 18\n",
      "Final Val Loss: 0.6885693073272705\n",
      "Final Val Loss: 0.6939618587493896\n",
      "Final Val Loss: 0.687221109867096\n",
      "Final Val Loss: 0.6858730316162109\n",
      "Final Val Loss: 0.6939618587493896\n",
      "Final Val Loss: 0.6926136612892151\n",
      "Final Val Loss: 0.6926137208938599\n",
      "Final Val Loss: 0.6966581344604492\n",
      "Final Training Loss: 0.6954055428504944\n",
      "Final Training Loss: 0.6943472623825073\n",
      "Final Training Loss: 0.6964637041091919\n",
      "Final Training Loss: 0.69117271900177\n",
      "Final Training Loss: 0.6911726593971252\n",
      "Final Training Loss: 0.6985801458358765\n",
      "Final Training Loss: 0.6943473815917969\n",
      "Final Training Loss: 0.6932891607284546\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "epoch number: 19\n",
      "Final Val Loss: 0.6858676671981812\n",
      "Final Val Loss: 0.6939624547958374\n",
      "Final Val Loss: 0.6872167587280273\n",
      "Final Val Loss: 0.6966608166694641\n",
      "Final Val Loss: 0.692613422870636\n",
      "Final Val Loss: 0.6885659098625183\n",
      "Final Val Loss: 0.6926133632659912\n",
      "Final Val Loss: 0.6939624547958374\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "def fit_multiprocessing(rank, flags):\n",
    "  fit(model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint,num_labels=2), \n",
    "      trainDataset = trainDataset, valDataset = valDataset, epochs = 20,  batchSize = 32, lr = lr)\n",
    "\n",
    "FLAGS = {}\n",
    "xmp.spawn(fit_multiprocessing, args =(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJuGeiqksH1S",
    "outputId": "3d76231a-f680-441b-bf95-bbb806a1e1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900],\n",
      "        [-0.3900,  0.3900]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "_model = DistilBertForSequenceClassification(\n",
    "    config = DistilBertConfig(num_labels = 2))\n",
    "\n",
    "checkpoint = torch.load('model.pt')\n",
    "_model.load_state_dict(checkpoint)\n",
    "\n",
    "_model.eval()\n",
    "\n",
    "valLoader = DataLoader(valDataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle = False, \n",
    "                        num_workers=0,\n",
    "                        drop_last=True)\n",
    "with torch.no_grad():\n",
    "  for batch in valLoader:\n",
    "    input_ids = batch['input_ids']\n",
    "\n",
    "    outputs = _model(input_ids)\n",
    "    break\n",
    "\n",
    "  print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF_VSL-1OA_t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN3mvpRymZpIZ5g8SH8la1L",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "SentimentAnalysisClassifierGPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ea7c3c54c0443494c22cb137fd0a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2fc6d53352f4c6fba1b78af87431534",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef4e0ee90da64dc6966aa60f0f27d86c",
      "value": 442
     }
    },
    "08796c2378c34273909c687e15f23736": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1010f13992f342c2a471e6cf919134cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "110e89a85c324de08bdcce38ff929c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6d80261d0cc4d0ba7000fb55f003908",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fa0bd7386c364017a288da7c731d8774",
      "value": " 268M/268M [00:12&lt;00:00, 22.0MB/s]"
     }
    },
    "158693a626d94eb0bb02e8c6cbfabb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "18154712e27d4a52857235032aad2809": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cf9d4fcf1c34b3abb133b7f2bcfbba1",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_158693a626d94eb0bb02e8c6cbfabb4d",
      "value": 267967963
     }
    },
    "23e92002b12e42be9b26530b4f740b8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49a2a7f1b3bb49f381a67e958818d08e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5205c88e70c845529bbd6bf58b4ff2a9",
      "value": " 442/442 [00:18&lt;00:00, 23.6B/s]"
     }
    },
    "30dc920a9df44393b5adc06e516f2286": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3459b8491b514d1abb6c48959e00596a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "359e9ce85cda46e88cb0963cca604f77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36423e60e056403ebce7d7851b838c8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d71dce6973034b04890c189c393ec01e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48055a56c33c4d8e835af2760e450fd2",
      "value": 231508
     }
    },
    "48055a56c33c4d8e835af2760e450fd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "49747ab5d4a144dabd9e610bfa04d37e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49a2a7f1b3bb49f381a67e958818d08e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51b9544a54ce4d2eadb0df71c7b8586f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c01e5c2aa98f44b39f7ca054f2507ef1",
       "IPY_MODEL_9d389861befe4ca9b019cec4fc66491d"
      ],
      "layout": "IPY_MODEL_7dff024bc83b45a4b8305b851dd0ad32"
     }
    },
    "5205c88e70c845529bbd6bf58b4ff2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c9316e621324ebb84a794a63737fff7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62786836aa534c28b0bcace022361552": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a895ccee3e94ac591e8f1bbaacf79ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cf9d4fcf1c34b3abb133b7f2bcfbba1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dff024bc83b45a4b8305b851dd0ad32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f2a90ee108b48258ef407c6be22302e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ec8fb94bd8b4325b491cb29f8366fb0",
       "IPY_MODEL_9dbb489d6bc543fa860295d4f1772548"
      ],
      "layout": "IPY_MODEL_5c9316e621324ebb84a794a63737fff7"
     }
    },
    "89e67e23cab346088fbc5f8bad9e0a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18154712e27d4a52857235032aad2809",
       "IPY_MODEL_110e89a85c324de08bdcce38ff929c66"
      ],
      "layout": "IPY_MODEL_7a895ccee3e94ac591e8f1bbaacf79ff"
     }
    },
    "9d389861befe4ca9b019cec4fc66491d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30dc920a9df44393b5adc06e516f2286",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_da0414b04a344364afb4cdbf471fdc75",
      "value": " 28.0/28.0 [00:15&lt;00:00, 1.77B/s]"
     }
    },
    "9dbb489d6bc543fa860295d4f1772548": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08796c2378c34273909c687e15f23736",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1010f13992f342c2a471e6cf919134cf",
      "value": " 466k/466k [00:17&lt;00:00, 26.9kB/s]"
     }
    },
    "9ec8fb94bd8b4325b491cb29f8366fb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49747ab5d4a144dabd9e610bfa04d37e",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3459b8491b514d1abb6c48959e00596a",
      "value": 466062
     }
    },
    "a23de6e2cc884ece9a389767153e3f19": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d433fd4fa74582927bf55bdbf31a00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a37bce5d99514843b0db3403983aad46": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6d80261d0cc4d0ba7000fb55f003908": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9092cb6bfed44f181e22c3e33be6bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_00ea7c3c54c0443494c22cb137fd0a73",
       "IPY_MODEL_23e92002b12e42be9b26530b4f740b8e"
      ],
      "layout": "IPY_MODEL_62786836aa534c28b0bcace022361552"
     }
    },
    "beffe4a4ee5e4b60977f47ec631beb7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c01e5c2aa98f44b39f7ca054f2507ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_359e9ce85cda46e88cb0963cca604f77",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2d433fd4fa74582927bf55bdbf31a00",
      "value": 28
     }
    },
    "ccaea8639e164503b45bea9deb5d90cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a37bce5d99514843b0db3403983aad46",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_beffe4a4ee5e4b60977f47ec631beb7d",
      "value": " 232k/232k [00:00&lt;00:00, 266kB/s]"
     }
    },
    "d46122f07afa4c09a42222dbb592b0ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36423e60e056403ebce7d7851b838c8d",
       "IPY_MODEL_ccaea8639e164503b45bea9deb5d90cc"
      ],
      "layout": "IPY_MODEL_a23de6e2cc884ece9a389767153e3f19"
     }
    },
    "d71dce6973034b04890c189c393ec01e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0414b04a344364afb4cdbf471fdc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef4e0ee90da64dc6966aa60f0f27d86c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f2fc6d53352f4c6fba1b78af87431534": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa0bd7386c364017a288da7c731d8774": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
