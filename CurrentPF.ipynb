{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CurrentPF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN5R+yZKjaQ0zSZFy47nXW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttb-folio/Pitchfork-Reviews/blob/main/CurrentPF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LENDdMahAd2",
        "outputId": "eb8230c6-5759-46c4-b9b2-f4fcec5833c3"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n",
        "\n",
        "!pip install efficientnet_pytorch > /dev/null\n",
        "!pip install albumentations > /dev/null\n",
        "!pip install transformers\n",
        "\n",
        "!git clone https://github.com/ttb-folio/Pitchfork-Reviews.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5116  100  5116    0     0   111k      0 --:--:-- --:--:-- --:--:--  111k\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Uninstalling torch-1.6.0a0+bf2bbd9:\n",
            "  Successfully uninstalled torch-1.6.0a0+bf2bbd9\n",
            "Uninstalling torchvision-0.7.0a0+a6073f0:\n",
            "  Successfully uninstalled torchvision-0.7.0a0+a6073f0\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "| [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==nightly+20200515) (1.19.5)\n",
            "Done updating TPU runtime\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+2b2085a\n",
            "    Uninstalling torch-xla-1.6+2b2085a:\n",
            "      Successfully uninstalled torch-xla-1.6+2b2085a\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==nightly+20200515) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "fatal: destination path 'Pitchfork-Reviews' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwWXhhf-gWI_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "from transformers import AutoTokenizer, set_seed, DistilBertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "assert os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMQAKtG2hxCH"
      },
      "source": [
        "content = pd.read_csv('Pitchfork-Reviews/Data/content.csv')\n",
        "reviews = pd.read_csv('Pitchfork-Reviews/Data/reviews.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KEzytZsivuq"
      },
      "source": [
        "#Sort by publish date so train, val, and test can be split in a way that maintains chronological order.\n",
        "reviewIdSorted = reviews.sort_values(by = 'pub_date').reset_index()['reviewid']\n",
        "\n",
        "#70/15/15 split for train/val/test\n",
        "numberIds = len(reviewIdSorted)\n",
        "valCutOff = numberIds//10*7\n",
        "testCutOff = valCutOff + numberIds//20*3\n",
        "trainIds = reviewIdSorted[0:valCutOff]\n",
        "valIds = reviewIdSorted[valCutOff:testCutOff]\n",
        "testIds = reviewIdSorted[testCutOff:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yR9aaGBi50h"
      },
      "source": [
        "#Join content and reviews dataframe on review id, keeping content (raw text of review) and score \n",
        "contentScorePairs = pd.merge(\n",
        "    content[['reviewid', 'content']],\n",
        "    reviews[['reviewid', 'score']],\n",
        "    on = 'reviewid')\n",
        "\n",
        "#Remove items with no written review\n",
        "contentScorePairs = contentScorePairs[~contentScorePairs['content'].isna()]\n",
        "\n",
        "trainContentScorePairs = contentScorePairs[contentScorePairs['reviewid'].isin(trainIds)]\n",
        "valContentScorePairs = contentScorePairs[contentScorePairs['reviewid'].isin(valIds)]\n",
        "testContentScorePairs = contentScorePairs[contentScorePairs['reviewid'].isin(testIds)]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFJ8QGC1i7uv"
      },
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "def tokenize(contentScorePair):\n",
        "    text = list(contentScorePair['content'])\n",
        "    tokenizedText = tokenizer(text, padding = True, truncation = True)\n",
        "    return tokenizedText\n",
        "\n",
        "xTrain = tokenize(trainContentScorePairs)\n",
        "xVal = tokenize(valContentScorePairs)\n",
        "xTest = tokenize(testContentScorePairs)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WXcSYei9Mt"
      },
      "source": [
        "#Convert review score (with range 0.0 - 10.0) into classes.\n",
        "#I choose to set the thresholds to be defined by the quantiles in the training set instead of even spacing.\n",
        "#Fewer than 25% of scores are less than 6.3, so it would not be very useful to distguish between below 2.5 and between 2.5 and 5. \n",
        "#Distinguishing between 6.3 to 7.2 and 7.2 to 7.9 is much more interesting.\n",
        "#I only use the training set because we don't know the true distribution of scores to be predicted.\n",
        "\n",
        "# vLowScoreLimit = trainContentScorePairs.score.quantile(.25) #6.3\n",
        "# lowScoreLimit = trainContentScorePairs.score.quantile(.5) #7.2\n",
        "# highScoreLimit = trainContentScorePairs.score.quantile(.75) #7.9\n",
        "# vHighScoreLimit = trainContentScorePairs.score.quantile(1) #10.0\n",
        "\n",
        "# def convertScore(score):\n",
        "#     if score <= vLowScoreLimit:\n",
        "#         y = 0\n",
        "#     elif score <= lowScoreLimit:\n",
        "#         y = 1\n",
        "#     elif score <= highScoreLimit:\n",
        "#         y = 2\n",
        "#     elif score <= vHighScoreLimit:\n",
        "#         y = 3\n",
        "#     else:\n",
        "#         y = 100000\n",
        "#     return y\n",
        "\n",
        "def convertScore(score):\n",
        "    if score <= 6: #trainContentScorePairs.score.quantile(.5):\n",
        "      y = 0\n",
        "    else:\n",
        "      y = 1\n",
        "    return y\n",
        "\n",
        "trainContentScorePairs['target'] = trainContentScorePairs['score'].transform(lambda x: convertScore(x))\n",
        "valContentScorePairs['target'] = valContentScorePairs['score'].transform(lambda x: convertScore(x))\n",
        "testContentScorePairs['target'] = testContentScorePairs['score'].transform(lambda x: convertScore(x))\n",
        "\n",
        "tTrain = list(trainContentScorePairs['target'].values)\n",
        "tVal = list(valContentScorePairs['target'].values)\n",
        "tTest = list(testContentScorePairs['target'].values)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv20_YEsi_I4"
      },
      "source": [
        "#Combine x and t values to make Datasets, for easy input into Torch DataLoader\n",
        "\n",
        "class reviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "trainDataset = reviewDataset(xTrain, tTrain)\n",
        "valDataset = reviewDataset(xVal, tVal)\n",
        "testDataset = reviewDataset(xTest, tTest)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge3sy-BaIKCK"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "def fit(model, trainDataset, valDataset, epochs = 1, batchSize = 32, lr = 1e-3, scheduler=None):\n",
        "  device = xm.xla_device()\n",
        "  model = model.to(device)\n",
        "  \n",
        "  trainSampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    trainDataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  trainLoader = DataLoader(trainDataset, \n",
        "                           batch_size=batchSize, \n",
        "                           sampler=trainSampler, \n",
        "                           num_workers=0, \n",
        "                           drop_last = True)\n",
        "  valLoader = DataLoader(valDataset, \n",
        "                         batch_size=batchSize, \n",
        "                         shuffle = False, \n",
        "                         num_workers=0,\n",
        "                         drop_last=True)\n",
        "  num_train_steps = int(len(trainDataset) / batchSize /xm.xrt_world_size()*epochs)\n",
        "  optimizer = AdamW(model.parameters(), lr = lr)\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer, num_warmup_steps = 0, num_training_steps = num_train_steps\n",
        "  )\n",
        "  def train_loop(trainLoader):\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    for batch in trainLoader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss = outputs[0]\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer, barrier = True)\n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "    print(f'Final Training Loss: {loss}')\n",
        "  def eval_loop(valLoader):\n",
        "    with torch.no_grad():\n",
        "      for batch in valLoader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "      print(f'Final Val Loss: {loss}')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(lr)\n",
        "    model.train()\n",
        "    para_loader = pl.ParallelLoader(trainLoader,[device])\n",
        "    train_loop(para_loader.per_device_loader(device))\n",
        "    del para_loader\n",
        "    \n",
        "    model.eval()\n",
        "    para_loader = pl.ParallelLoader(valLoader, [device])\n",
        "    eval_loop(para_loader.per_device_loader(device))\n",
        "    del para_loader\n",
        "    xm.save(model.state_dict(), \"model.pt\")\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVIK0lZEHn6s",
        "outputId": "78c53f59-bbff-4e7a-f942-03d93dcf1e5c"
      },
      "source": [
        "lr = 5e-3\n",
        "def fit_multiprocessing(rank, flags):\n",
        "  fit(model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint,num_labels=2), \n",
        "      trainDataset = trainDataset, valDataset = valDataset, epochs = 5,  batchSize = 32, lr = lr)\n",
        "\n",
        "FLAGS = {}\n",
        "xmp.spawn(fit_multiprocessing, args =(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "Final Training Loss: 4.011714458465576\n",
            "Final Training Loss: 2.386448860168457\n",
            "Final Training Loss: 5.173232078552246\n",
            "Final Training Loss: 4.90132474899292\n",
            "Final Training Loss: 4.715847969055176\n",
            "Final Training Loss: 4.835206031799316\n",
            "Final Training Loss: 6.707940578460693\n",
            "Final Training Loss: 5.3618669509887695\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "Final Val Loss: 1.1402983665466309\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "Final Training Loss: 6.347614288330078\n",
            "Final Training Loss: 9.70417594909668\n",
            "Final Training Loss: 8.661746978759766\n",
            "Final Training Loss: 7.475641250610352\n",
            "Final Training Loss: 6.312149524688721\n",
            "Final Training Loss: 6.382071495056152\n",
            "Final Training Loss: 4.2785234451293945\n",
            "Final Training Loss: 3.2049314975738525\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "Final Val Loss: 0.8499675989151001\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "Final Training Loss: 0.5478062033653259\n",
            "Final Training Loss: 0.621130645275116\n",
            "Final Training Loss: 0.47448164224624634\n",
            "Final Training Loss: 0.5233646631240845\n",
            "Final Training Loss: 0.5722476840019226\n",
            "Final Training Loss: 0.5233646631240845\n",
            "Final Training Loss: 0.4744816720485687\n",
            "Final Training Loss: 0.5478062033653259\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "Final Val Loss: 0.4008273184299469\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "Final Training Loss: 0.5166570544242859\n",
            "Final Training Loss: 0.5690352320671082\n",
            "Final Training Loss: 0.542846143245697\n",
            "Final Training Loss: 0.6214134693145752\n",
            "Final Training Loss: 0.46427881717681885\n",
            "Final Training Loss: 0.5428462028503418\n",
            "Final Training Loss: 0.46427881717681885\n",
            "Final Training Loss: 0.5166570544242859\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "Final Val Loss: 0.385195255279541\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "0.005\n",
            "Final Training Loss: 0.5130133032798767\n",
            "Final Training Loss: 0.5130133032798767\n",
            "Final Training Loss: 0.540226936340332\n",
            "Final Training Loss: 0.4585860073566437\n",
            "Final Training Loss: 0.621867835521698\n",
            "Final Training Loss: 0.45858603715896606\n",
            "Final Training Loss: 0.5674405694007874\n",
            "Final Training Loss: 0.540226936340332\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n",
            "Final Val Loss: 0.3769232928752899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJuGeiqksH1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89126347-2a3c-46d9-f21c-8c7f05127a3a"
      },
      "source": [
        "from transformers import DistilBertConfig\n",
        "\n",
        "_model = DistilBertForSequenceClassification(\n",
        "    config = DistilBertConfig(num_labels = 2))\n",
        "\n",
        "checkpoint = torch.load('model.pt')\n",
        "_model.load_state_dict(checkpoint)\n",
        "\n",
        "_model.eval()\n",
        "\n",
        "valLoader = DataLoader(valDataset, \n",
        "                        batch_size=32, \n",
        "                        shuffle = False, \n",
        "                        num_workers=0,\n",
        "                        drop_last=True)\n",
        "with torch.no_grad():\n",
        "  for batch in valLoader:\n",
        "    input_ids = batch['input_ids']\n",
        "\n",
        "    outputs = _model(input_ids)\n",
        "    break\n",
        "\n",
        "  print(outputs[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256],\n",
            "        [-0.7256,  0.7256]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_VSL-1OA_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}